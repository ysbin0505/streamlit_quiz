# datalyManager.py
import streamlit as st
import zipfile
import tempfile
import os
import json
import pandas as pd
from collections import defaultdict
from io import BytesIO

from openpyxl.styles import Alignment, PatternFill, Font, Border, Side

# ê¸°ì¡´ ê¸°ëŠ¥ìš© ëª¨ë“ˆ
from newspaper_eval_merged import json_to_excel_stacked
from newspaper_eval_json import merge_newspaper_eval

# ========== ê³µí†µ ìŠ¤íƒ€ì¼ ==========
st.markdown("""
    <style>
    .main-title {font-size:2.1rem; font-weight:bold; color:#174B99; margin-bottom:0;}
    .sub-desc {font-size:1.1rem; color:#222;}
    .logo {height:60px; margin-bottom:15px;}
    .footer {color:#999; font-size:0.9rem; margin-top:40px;}
    div.stButton > button:first-child {background:#174B99; color:white; font-weight:bold; border-radius:8px;}
    .stTabs [data-baseweb="tab-list"] {background:#F6FAFD;}
    </style>
""", unsafe_allow_html=True)

# ìƒë‹¨ ë¸Œëœë“œ/ë¡œê³ 
col1, col2 = st.columns([0.15, 0.85])
with col1:
    st.image("https://static.streamlit.io/examples/cat.jpg", width=55)
with col2:
    st.markdown('<div class="main-title">Dataly Manager</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-desc">ì—…ë¬´ ìë™í™”, í‰ê°€ ë°ì´í„° ë³€í™˜, ê´€ë¦¬, ìˆ˜í•© ì›¹ì•±</div>', unsafe_allow_html=True)

# ================== í‘œ ë³€í™˜(í‘œ_ë³€í™˜.py)ì—ì„œ ì“°ëŠ” í—¬í¼/ë§¤í•‘ ==================
ref_map = {
    "table_ref": "í‘œ ì„¤ëª… ë¬¸ì¥",
    "row_ref": "í–‰ ì„¤ëª… ë¬¸ì¥",
    "col_ref": "ì—´ ì„¤ëª… ë¬¸ì¥",
    "cell_ref": "ë¶ˆì—°ì† ì˜ì—­ ì„¤ëª… ë¬¸ì¥"
}

def extract_mdfcn_values(obj, sep="\n"):
    """
    mdfcn_infosê°€ ë¬¸ìì—´/ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ ë“± ì–´ë–¤ í˜•íƒœë“ 
    ë‚´ë¶€ì˜ 'value' ê°’ë§Œ ì¶”ì¶œí•´ ì¤‘ë³µ ì œê±°(ë“±ì¥ ìˆœì„œ ìœ ì§€) í›„ sepë¡œ ì—°ê²°.
    - 'mdfcn_memo'ê°€ JSON ë¬¸ìì—´ì¸ ê²½ìš°ë„ íŒŒì‹±í•´ì„œ 'value'ë§Œ ì¶”ì¶œ.
    - 'table_ref', 'row_ref', 'col_ref', 'cell_ref' ë“± type íƒœê·¸ ë¬¸ìì—´ì€ ë¬´ì‹œ.
    """
    values = []
    TYPE_TAGS = {"table_ref", "row_ref", "col_ref", "cell_ref"}

    def _dedup_keep_order(items):
        seen = set()
        out = []
        for it in items:
            if it and it not in seen:
                seen.add(it)
                out.append(it)
        return out

    def _walk(x):
        if x is None:
            return
        if isinstance(x, str):
            s = x.strip()
            if not s:
                return
            if s[:1] in ("[", "{"):
                try:
                    _walk(json.loads(s))
                    return
                except Exception:
                    if s not in TYPE_TAGS:
                        values.append(s)
                    return
            if s not in TYPE_TAGS:
                values.append(s)
            return
        if isinstance(x, dict):
            v = x.get("value")
            if isinstance(v, str):
                v = v.strip()
                if v:
                    values.append(v)
            mm = x.get("mdfcn_memo")
            if isinstance(mm, str):
                mm_s = mm.strip()
                if mm_s:
                    try:
                        _walk(json.loads(mm_s))
                    except Exception:
                        pass
            for k, sub in x.items():
                if k in ("value", "mdfcn_memo"):
                    continue
                if isinstance(sub, (list, dict)):
                    _walk(sub)
            return
        if isinstance(x, (list, tuple)):
            for it in x:
                _walk(it)
            return

    _walk(obj)
    return sep.join(_dedup_keep_order([v for v in values if isinstance(v, str)]))

def extract_url(meta):
    if isinstance(meta, dict):
        u = meta.get("url", "")
        if isinstance(u, list):
            return u[0] if u else ""
        return str(u or "")
    return ""

def convert_table_json_to_excel_bytes(data) -> bytes:
    """
    í‘œ_ë³€í™˜.pyì˜ ì „ì²´ ë¡œì§ì„ ë©”ëª¨ë¦¬ ë²„ì „ìœ¼ë¡œ ë³€í™˜.
    ì…ë ¥: íŒŒì‹±ëœ JSON(dict)
    ì¶œë ¥: ìƒì„±ëœ XLSXì˜ bytes
    """
    rows = []
    group_counts = defaultdict(int)

    for doc in data.get("document", []):
        doc_id = doc.get("id", "")
        worker = str(doc.get("worker_id_cnst") or "").strip()
        metadata = doc.get("metadata", {})

        # mdfcn_infos ì›ë³¸ ìˆ˜ì§‘ (í‚¤ ë³€ë™ ëŒ€ë¹„)
        mdfcn_raw = doc.get("mdfcn_infos", doc.get("mdfcn_memo", []))
        mdfcn_text = extract_mdfcn_values(mdfcn_raw, sep="\n")

        for ex in doc.get("EX", []):
            ref_type = ex.get("reference", {}).get("reference_type", "")
            exp_list = ex.get("exp_sentence", [])

            for exp in exp_list:
                sentence = ""
                try:
                    sent_list = exp.get("ì„¤ëª… ë¬¸ì¥", [])
                    sentence = sent_list[0] if sent_list else ""
                except Exception:
                    sentence = ""

                rows.append({
                    "id": doc_id,
                    "worker_id_cnst": worker,
                    "ìœ í˜•": ref_map.get(ref_type, ref_type),
                    "ì„¤ëª… ë¬¸ì¥": sentence,
                    "metadata": json.dumps(metadata, ensure_ascii=False, indent=2),
                    "mdfcn_infos": mdfcn_text,
                    "url": extract_url(metadata),
                })
                group_counts[doc_id] += 1

    df = pd.DataFrame(
        rows,
        columns=["id", "worker_id_cnst", "ìœ í˜•", "ì„¤ëª… ë¬¸ì¥", "metadata", "mdfcn_infos"]
    ).fillna("")

    output = BytesIO()
    with pd.ExcelWriter(output, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="sheet1")
        ws = writer.sheets["sheet1"]

        # ì—´ ë„ˆë¹„
        widths = {"A": 18, "B": 16, "C": 13, "D": 80, "E": 50, "F": 50}
        for col, w in widths.items():
            ws.column_dimensions[col].width = w

        # ë¨¸ë¦¬ê¸€ ìŠ¤íƒ€ì¼
        header_fill = PatternFill("solid", fgColor="D9E1F2")
        header_font = Font(bold=True)
        for cell in ws[1]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)

        # metadata, mdfcn_infos í—¤ë” ê°•ì¡°
        ws["E1"].fill = PatternFill("solid", fgColor="BDD7EE")
        ws["F1"].fill = PatternFill("solid", fgColor="FCE4D6")

        thin = Side(style="thin", color="999999")
        border = Border(left=thin, right=thin, top=thin, bottom=thin)

        max_row = ws.max_row
        max_col = ws.max_column

        for r in range(2, max_row + 1):
            for c in range(1, max_col + 1):
                cell = ws.cell(row=r, column=c)
                if c >= 4:  # D,E,F
                    cell.alignment = Alignment(wrap_text=True, vertical="top")
                else:
                    cell.alignment = Alignment(vertical="top")
                cell.border = border

        # id ë¸”ë¡ ë³‘í•©: A, B, E, F
        cur_row = 2
        for doc_id, count in group_counts.items():
            if count > 1:
                for col in (1, 2, 5, 6):
                    ws.merge_cells(
                        start_row=cur_row, start_column=col,
                        end_row=cur_row + count - 1, end_column=col
                    )
                    top_cell = ws.cell(row=cur_row, column=col)
                    top_cell.alignment = Alignment(vertical="top", wrap_text=True)
                    for rr in range(cur_row, cur_row + count):
                        ws.cell(row=rr, column=col).border = border
            cur_row += count

        # í•˜ì´í¼ë§í¬: ë³‘í•© ì²« í–‰ë§Œ
        first_row_for_id = {}
        for idx, row in enumerate(rows, start=2):
            first_row_for_id.setdefault(row["id"], idx)
        for idx, row in enumerate(rows, start=2):
            url = row.get("url", "")
            if not url:
                continue
            if idx != first_row_for_id[row["id"]]:
                continue
            meta_cell = ws.cell(row=idx, column=5)  # E
            meta_cell.hyperlink = url

        # í–‰ ë†’ì´: D/E/F ê°œí–‰ ìˆ˜ ê¸°ì¤€
        for r in range(2, max_row + 1):
            max_lines = 1
            for c in (4, 5, 6):
                val = ws.cell(row=r, column=c).value
                if isinstance(val, str) and "\n" in val:
                    lines = val.count("\n") + 1
                    if lines > max_lines:
                        max_lines = lines
            ws.row_dimensions[r].height = min(15 + (max_lines - 1) * 12, 200)

        # í‹€ ê³ ì •
        ws.freeze_panes = "A2"

    output.seek(0)
    return output.getvalue()

# ========== íƒ­ ==========
tabs = st.tabs([
    "ğŸ  í™ˆ",
    "ğŸ“° ì‹ ë¬¸í‰ê°€ ìˆ˜í•©",
    "ğŸ’¬ ëŒ€í™”í‰ê°€ ë³‘í•©",
    "ğŸ“¦ ì‹ ë¬¸í‰ê°€ ë³‘í•©",
    "ğŸ“Š í‘œ ë³€í™˜ (JSONâ†’Excel)"  # ìƒˆ íƒ­ ì¶”ê°€
])

# í™ˆ
with tabs[0]:
    st.markdown("#### ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤!<br>ì•„ë˜ íƒ­ì—ì„œ ê¸°ëŠ¥ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.", unsafe_allow_html=True)
    st.markdown("""
    - ğŸ“° ì‹ ë¬¸í‰ê°€ ìˆ˜í•©: ì‹ ë¬¸ JSONì„ ì—‘ì…€ë¡œ ë³€í™˜
    - ğŸ’¬ ëŒ€í™”í‰ê°€ ë³‘í•©: ëŒ€í™” í‰ê°€ ë³‘í•© (ì¶”ê°€ì˜ˆì •)
    - ğŸ“¦ ì‹ ë¬¸í‰ê°€ ë³‘í•©: A/BíŒ€ JSON ë¬¶ìŒ ë³‘í•© ZIP ìƒì„±
    - ğŸ“Š í‘œ ë³€í™˜: ë‹¨ì¼ JSONì„ í‘œ í˜•íƒœ ì—‘ì…€ë¡œ ë³€í™˜
    """)

# ì‹ ë¬¸í‰ê°€ ìˆ˜í•©
with tabs[1]:
    st.header("ğŸ“° ì‹ ë¬¸í‰ê°€ JSON â†’ ì—‘ì…€ ìë™ ìˆ˜í•©ê¸°")
    st.info("ì•„ë˜ ìˆœì„œëŒ€ë¡œ ì—…ë¡œë“œ ë° ì‹¤í–‰ì„ ì§„í–‰í•˜ì„¸ìš”.")
    uploaded_zip = st.file_uploader("1. í‰ê°€ ë°ì´í„° ZIP ì—…ë¡œë“œ (í´ë”ë¥¼ ì••ì¶•)", type=["zip"], key="file_upload_zip_sum")
    sum_week_num = st.number_input("2. ìˆ˜í•©í•  ì£¼ì°¨ (ì˜ˆ: 1)", min_value=1, step=1, value=1, key="sum_week_num")
    storage_folder = st.selectbox("3. storage í´ë”ëª… ì„ íƒ", ["storage0", "storage1"], key="sum_storage_folder")
    run_btn = st.button("ì‹¤í–‰ (ì—‘ì…€ ë³€í™˜)", key="run_newspaper_sum")

    if uploaded_zip and run_btn:
        with tempfile.TemporaryDirectory() as temp_dir:
            tmp_zip_path = os.path.join(temp_dir, "data.zip")
            with open(tmp_zip_path, "wb") as f:
                f.write(uploaded_zip.read())
            with zipfile.ZipFile(tmp_zip_path, "r") as zip_ref:
                zip_ref.extractall(temp_dir)
            folder_list = [f for f in os.listdir(temp_dir) if os.path.isdir(os.path.join(temp_dir, f))]
            if not folder_list:
                st.error("ì••ì¶•íŒŒì¼ ë‚´ë¶€ì— í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. í´ë”ì§¸ ì••ì¶•í•œ zipë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
            else:
                root_path = os.path.join(temp_dir, folder_list[0])
                st.info("ì—‘ì…€ ë³€í™˜ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ìˆ˜ì´ˆ~ìˆ˜ì‹­ì´ˆ ì†Œìš”)")
                json_to_excel_stacked(root_path, sum_week_num, storage_folder)
                excel_path = os.path.join(root_path, "summary_eval_all.xlsx")
                if os.path.exists(excel_path):
                    with open(excel_path, "rb") as f:
                        st.success("ì—‘ì…€ ë³€í™˜ ì™„ë£Œ! ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                        st.download_button(
                            label="summary_eval_all.xlsx ë‹¤ìš´ë¡œë“œ",
                            data=f,
                            file_name="summary_eval_all.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                        )
                else:
                    st.error("ì—‘ì…€ íŒŒì¼ ìƒì„± ì‹¤íŒ¨. ë‚´ë¶€ ì˜¤ë¥˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    else:
        st.info("ZIP, ì£¼ì°¨, í´ë”ëª… ì…ë ¥ í›„ [ì‹¤í–‰]ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# ëŒ€í™”í‰ê°€ ë³‘í•© (ì¤€ë¹„ì¤‘)
with tabs[2]:
    st.header("ğŸ’¬ ëŒ€í™”í‰ê°€ ë³‘í•© (ì¤€ë¹„ì¤‘)")
    st.info("ì´ ê¸°ëŠ¥ì€ ê³§ ì¶”ê°€ë©ë‹ˆë‹¤. ì›í•˜ì‹œëŠ” ê¸°ëŠ¥ì´ ìˆë‹¤ë©´ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.")

# ì‹ ë¬¸í‰ê°€ ë³‘í•©
with tabs[3]:
    st.header("ğŸ“¦ ì‹ ë¬¸í‰ê°€ JSON ë³‘í•©")
    st.info("ZIP ë‚´ 'A/AíŒ€', 'B/BíŒ€' í´ë”ì™€ JSON íŒŒì¼ì´ ì¡´ì¬í•´ì•¼ í•˜ë©°, ë³‘í•© ê²°ê³¼ëŠ” ìë™ìœ¼ë¡œ ì••ì¶•íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    uploaded_zip = st.file_uploader("ë³‘í•©í•  ì‹ ë¬¸ ì›ë³¸ ZIP ì—…ë¡œë“œ (A/BíŒ€ í¬í•¨ í´ë”)", type=["zip"], key="merge_zip_upload")
    merge_week_num = st.number_input("ë³‘í•©í•  ì£¼ì°¨ (ì˜ˆ: 1)", min_value=1, step=1, value=1, key="merge_week_num")
    files_per_week = st.number_input("ë³‘í•©í•  íŒŒì¼ ìˆ˜ (ë³´í†µ 102)", min_value=1, step=1, value=102, key="merge_files_per_week")
    run_merge_btn = st.button("ì‹ ë¬¸í‰ê°€ ë³‘í•© ì‹¤í–‰", key="run_newspaper_merge")

    if uploaded_zip and run_merge_btn:
        with tempfile.TemporaryDirectory() as temp_dir:
            tmp_zip_path = os.path.join(temp_dir, "src.zip")
            with open(tmp_zip_path, "wb") as f:
                f.write(uploaded_zip.read())
            with zipfile.ZipFile(tmp_zip_path, "r") as zip_ref:
                zip_ref.extractall(temp_dir)

            st.write("ì••ì¶• í•´ì œ í›„ ì„ì‹œí´ë” ëª©ë¡:", os.listdir(temp_dir))
            candidate_dirs = [os.path.join(temp_dir, d) for d in os.listdir(temp_dir) if os.path.isdir(os.path.join(temp_dir, d))]
            if not candidate_dirs:
                st.error("ì••ì¶• ë‚´ë¶€ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ZIP í´ë” êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            else:
                base_dir = candidate_dirs[0]
                st.write("ì„ íƒëœ base_dir:", base_dir)
                st.write("base_dir í´ë” ëª©ë¡:", os.listdir(base_dir))

                with st.spinner("ë³‘í•© ì¤‘ì…ë‹ˆë‹¤..."):
                    msg, output_dir, out_zip_path = merge_newspaper_eval(
                        week_num=int(merge_week_num),
                        files_per_week=int(files_per_week),
                        base_dir=base_dir
                    )
                st.success(f"ë³‘í•© ê²°ê³¼: {msg}")
                with open(out_zip_path, "rb") as f:
                    st.download_button(
                        label=f"{merge_week_num}ì£¼ì°¨ ë³‘í•© JSON ZIP ë‹¤ìš´ë¡œë“œ",
                        data=f,
                        file_name=f"merged_{merge_week_num}ì£¼ì°¨.zip",
                        mime="application/zip"
                    )
    else:
        st.info("ZIP íŒŒì¼, ì£¼ì°¨, íŒŒì¼ ìˆ˜ ì…ë ¥ í›„ ì‹¤í–‰ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# ğŸ“Š í‘œ ë³€í™˜ (JSONâ†’Excel) - ìƒˆ íƒ­
with tabs[4]:
    st.header("ğŸ“Š í‘œ ë³€í™˜ (ë‹¨ì¼ JSON â†’ Excel)")
    st.info("project_xxx.json í•œ ê°œë¥¼ ì—…ë¡œë“œí•˜ë©´, í‘œ í˜•íƒœ ì—‘ì…€ë¡œ ë³€í™˜í•˜ì—¬ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    uploaded_json = st.file_uploader("JSON ì—…ë¡œë“œ (project_*.json)", type=["json"], key="table_json_upload")
    run_table_btn = st.button("ì—‘ì…€ ë³€í™˜ ì‹¤í–‰", key="run_table_convert")

    if uploaded_json and run_table_btn:
        try:
            data = json.load(uploaded_json)
        except Exception as e:
            st.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        else:
            if not isinstance(data, dict) or "document" not in data:
                st.error("JSON ìµœìƒìœ„ì— 'document' ë°°ì—´ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            else:
                with st.spinner("ì—‘ì…€ ìƒì„± ì¤‘..."):
                    xlsx_bytes = convert_table_json_to_excel_bytes(data)
                st.success("ì—‘ì…€ ìƒì„± ì™„ë£Œ! ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                st.download_button(
                    label="í‘œ_ë³€í™˜.xlsx ë‹¤ìš´ë¡œë“œ",
                    data=xlsx_bytes,
                    file_name="í‘œ_ë³€í™˜.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
    else:
        st.info("JSON íŒŒì¼ ì—…ë¡œë“œ í›„ [ì—‘ì…€ ë³€í™˜ ì‹¤í–‰]ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")

# í•˜ë‹¨ í‘¸í„°
st.markdown("""
<hr>
<div class="footer">
ë¬¸ì˜: ê²€ì¦ ì—”ì§€ë‹ˆì–´ | Powered by Streamlit<br>
Copyright &copy; 2025. All rights reserved.
</div>
""", unsafe_allow_html=True)
